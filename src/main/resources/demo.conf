demo {

  // hadoop 配置信息
  hadoop {
    mapreduce.input.fileinputformat.input.dir.recursive = true // 递归
    input.file.filter = ".*" // 文件正则
    skip.header.line.count = 0 //跳过文件前几行
  }

  // spark 配置信息
  spark {
    spark.serializer = "org.apache.spark.serializer.KryoSerializer"
  }

  data.input.path = "/tmp/input/demo" // 数据目录
  file.text.decode = "UTF-8" // 字符解码
  data.input.line.regex.str = ".*" // 行的正则匹配表达式，可以为空
  data.input.line.regex.err.exit = false // 正则不匹配则异常退出


  columns.split.type = "regex" // 字段分割类型，fixed:定长,splitstr:字符分割,regex:正则分割
  // 字符分割，参数
  columns.split.splitstr {
    str = "\\|" // 字段分隔符
  }
  // 定长分割，参数
  columns.split.fixed {
    columns.length = "1,1,3,1,5,4,10,23" // 字段定长分割，如：10,12,34
  }
  // 正则表达式分割，参数
  columns.split.regex {
    // 正则表达式
    regex.str = "(\\w+)\\|(\\d+)\\|([+-]?\\d*\\.\\d+)\\|(\\d+)\\|([+-]?\\d*\\.\\d+)\\|(\\w+)\\|([\\d\\s-]+)\\|([\\d-\\s:\\.]+)"
  }

  columns.fixed.length = -1 // 字段长度，用于分割后校验,-1:不校验

  columns.format {
    date.6 = "yyyy-MM-dd" // 日期格式化,字段序号
    date.7 = "yyyy-MM-dd HH:mm:ss.S" // 日期格式化,字段序号
  }


  data.store.type = "hive" // 数据存储类型,hive：存入hive表，text:数据目录
  // 存入hive
  data.store.hive.tablename = "default.demo" // 类型为hive：表名

  // 文本输出
  data.store.text.output.path = "/tmp/output/demo" // 类型为text：输出目录
  data.store.text.columns.splitstr = "&" // 存储的分割符
}

